{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Data Processor</span> for DYMF Strategy\n",
    "\n",
    "This sheet starts the process of coding a backtest for a strategy picking stocks based on the ratio of the book value of equity to the market value of equity. The first step is to load data on both accounting numbers and stock returns, and organize it so we can access it as we backtest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processor required functions\n",
    "\n",
    "According to the pseudo-code above, the <span style=\"color:green\">Data Processor</span> needs to do the following tasks:\n",
    "1. Load all the necessary raw data (in constructor)\n",
    "1. Return an array of unique dates in the raw data (`unique_dates()`)\n",
    "1. For a given date, return a signal DataFrame containing all the latest signals for the appropriate universe of securities (`signal_df_for_date(date)`)\n",
    "1. For a given date, return a price DataFrame containing the latest prices for all securities potentially in the portfolio, including those not in the current investable universe (`price_df_for_date(date)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define class here\n",
    "class MSLDataProcessor():\n",
    "    \n",
    "    # Path to where we store the data\n",
    "    data_folder_path = Path('Downloads/School Stuff/MSF/Spring/Portfolio Management/Backtest Demo/Data') \n",
    "    \n",
    "    # Number of days between quarterly earnings announcement and when we can use data\n",
    "    day_lag = 1\n",
    "    trade_lag = 252\n",
    "    holding_period = 90\n",
    "     # Limit for Market cap\n",
    "    min_market_cap = 5000000000\n",
    "    # Minimum share price to open a new position\n",
    "    min_share_price = 3.0\n",
    "    \n",
    "    # Constructor, loads/cleans/merges data as needed\n",
    "    def __init__(self):\n",
    "        self.price_df = pd.read_stata('momentumfinal.dta')\n",
    "        #  use the same code as HW3, prc = abs(prc), etc. Anything to do with price_df should be almost identical\n",
    "        self.price_df = self.price_df.rename(columns={'SHROUT':'shrout','PRC':'prc','PERMNO':'permno','RET':'ret'})\n",
    "        \n",
    "        # Prices sometimes negative to indicate no volume at closing auction\n",
    "        # In these cases, price = -0.5*(bid+ask)\n",
    "        # But we don't use that information and so want prices to always be positive\n",
    "        # See http://www.crsp.org/products/documentation/data-definitions-p\n",
    "        self.price_df['prc'] = np.absolute(self.price_df['prc'])\n",
    "        \n",
    "        # Add next-days return as a new column 'ret_next'\n",
    "        # Use the safe_lead_lag: want lead return but only when permno the same\n",
    "        self.price_df.loc[:,'ret_next'] = safe_lead_lag(self.price_df.loc[:,'ret'],self.price_df.loc[:,'permno'],self.holding_period)\n",
    "        \n",
    "        # Load the data for the Momentum signal\n",
    "        # Daily data from 2019-2023 from WRDS stored in momentum.dta\n",
    "        self.signal_df = pd.read_stata('momentumfinal.dta')\n",
    "        self.signal_df = self.signal_df.rename(columns={'SHROUT':'shrout','PRC':'prc','PERMNO':'permno','RET':'ret'})\n",
    "        self.signal_df['date_year_lag'] = safe_lead_lag(self.price_df.loc[:,'date'],self.price_df.loc[:,'permno'],self.trade_lag)\n",
    "        #self.signal_df['max_date'] = safe_lead_lag(self.price_df.loc[:,'date'],self.price_df.loc[:,'permno'],-self.holding_period)\n",
    "        self.signal_df['price_252_days_ago'] = safe_lead_lag(self.price_df.loc[:,'prc'],self.price_df.loc[:,'permno'],-self.trade_lag)\n",
    "\n",
    "        # compute gross_ret column = 1+ret\n",
    "        self.signal_df['gross_ret'] = self.signal_df['ret'] + 1\n",
    "        \n",
    "        # add momentum column to signal_df\n",
    "        # make sure it's sorted by permno first then date\n",
    "        # compute momentum column = product of gross_ret for rows r-252 through r-1, making sure to only include values where all 252 are from the same permno\n",
    "        self.signal_df = self.signal_df.sort_values(['permno', 'date'])\n",
    "\n",
    "        # Calculate the rolling product of the past 252 days of 'gross_ret' for each 'permno'\n",
    "        # We shift the result to exclude the current day's return\n",
    "        # Should this shift be positive or negative? I think that it should be negative because prior days are above in the dataframe and shift -1 will shift up. \n",
    "        self.signal_df['momentum'] = self.signal_df.groupby('permno')['gross_ret'].transform(\n",
    "        lambda x: x.shift(-1).rolling(window=252, min_periods=252).apply(np.prod, raw=True))\n",
    "\n",
    "        # Compute the momentum as the rolling product divided by the price from 252 days ago\n",
    "        self.signal_df['momentum'] = self.signal_df['momentum'] / self.signal_df['price_252_days_ago']\n",
    "\n",
    "        #all_signal_df['momentum'] = all_signal_df.groupby(['permno','date']).apply(lambda x: x['gross_ret'].shift(1).rolling(252, min_periods=252).apply(np.prod) / x['prc'].shift(252)).reset_index(level=0,drop=True)\n",
    "\n",
    "        \n",
    "    # Returns an array with the unique dates for which we have loaded data\n",
    "    # Uses from the price_df since that's how frequency we can update portfolio value\n",
    "    # Filters all dates in price_df to return only dates for which we have signals as well\n",
    "    def unique_dates(self):\n",
    "        price_dates = pd.Series( np.sort(self.price_df.loc[:,'date'].unique()) )\n",
    "        #price_dates = price_dates.iloc['']\n",
    "        #min_signal_date = self.signal_df.loc[:,'date'].min() + np.timedelta64(self.year_lag,'D')\n",
    "        min_signal_date = self.signal_df.loc[:,'date_year_lag'].min()\n",
    "        #min_signal_date =  (safe_lead_lag(self.signal_df.loc[0,'date'],self.signal_df.loc[:,'permno'],252)).asint()\n",
    "        max_signal_date =  self.signal_df.loc[:,'date'].max() - np.timedelta64(self.holding_period,'D')\n",
    "        return price_dates[ (price_dates >= min_signal_date) & (price_dates <= max_signal_date) ].array\n",
    "  \n",
    "    # Returns a DataFrame containing one row for all securities in price_df as of date.\n",
    "    # Columns must include:\n",
    "    # - 'date': date on which price data observed\n",
    "    # - 'security_id': a security identifier\n",
    "    # - 'prc': price on date\n",
    "    # - 'ret': return from previous date to date\n",
    "    # Ignores liquidity and future-return availability requirements\n",
    "    # To be used only for closing decisions and execution decisions\n",
    "    # Some of the returned stocks cannot be traded\n",
    "    # Do not be picky with this just return everything\n",
    "\n",
    "    def price_df_for_date(self,date):\n",
    "        price_date_df = self.price_df.loc[self.price_df.loc[:,'date'] == date,:]\n",
    "        return price_date_df.rename(columns={'permno':'security_id'}) \n",
    "    \n",
    "    # Returns a DataFrame where each row is a security in the strategy's universe,\n",
    "    # Columes must include:\n",
    "    # - 'date': date on which price data observed\n",
    "    # - 'security_id': a security identifier\n",
    "    # - whatever signals the trading rule needs to decide which securities to open new positions in\n",
    "    #   - In this case, return cshoq, prccq, and ceqq so trading rule can compute B/M ratio\n",
    "    #\n",
    "    # Also responsible for applying whatever liquidity filters are wanted to narrow universe,\n",
    "    # and check that we have future return data (no point in backtesting if we don't know what happens next)\n",
    "    # Be picky here and this is where we will be paring down the data\n",
    "    def signal_df_for_date(self,date):\n",
    "        # find set of permnos considered tradeable as of date \n",
    "        # start with all rows return_df on date with non-nan and non-infinite ret_next\n",
    "        date_price_df = self.price_df.loc[ self.price_df.loc[:,'date'] == date,:]\n",
    "        date_price_df = date_price_df.loc[ np.isfinite(self.price_df.loc[:,'ret_next']),:]\n",
    "        date_price_df = date_price_df.drop(columns=['ret_next','COMNAM','TICKER','shrout'])\n",
    "        \n",
    "        # now signal data\n",
    "        # first only look at data prior to the date of the signal\n",
    "        all_past_signal_df = self.signal_df.loc[(self.signal_df.loc[:,'date'] < date ),:]\n",
    "        all_past_signal_df = all_past_signal_df.drop(columns=['date_year_lag','COMNAM','TICKER','shrout','date','prc','ret'])\n",
    "        #all_past_signal_df = all_past_signal_df.rename(columns = {'date':'date-1','prc':'prc-1','ret':'ret-1'})\n",
    "        \n",
    "        # then grab only the latest observation for each permno\n",
    "        latest_signal_df = all_past_signal_df.groupby('permno').last()\n",
    "        \n",
    "        # now merge with return data and return        \n",
    "        merged_df = date_price_df.merge(latest_signal_df,on='permno',how='inner')\n",
    "        merged_df = merged_df.rename(columns={'permno':'security_id'})  # use permno as our security_id\n",
    "        \n",
    "        # filter by liquidity requirements\n",
    "        merged_df = self.liquidity_filter(merged_df)\n",
    "        \n",
    "        # and return without the ret_next column so backtests don't cheat by using it\n",
    "        return merged_df\n",
    "        \n",
    "    # with all observations deemed too illiquid removed\n",
    "    # Liquidity requirements:\n",
    "    #  - price >= $3\n",
    "    # add in market cap limiter here too\n",
    "    def liquidity_filter(self,df):\n",
    "        return df.loc[ df.loc[:,'prc'] >= self.min_share_price,:]\n",
    "        \n",
    "###################################################################\n",
    "# Helper methods, do not modify\n",
    "###################################################################\n",
    "\n",
    "# Function safe_lead_lag returns a new Series with the lead/lagged values\n",
    "#  but only when a group is the same for the lead/lag\n",
    "# Inputs:\n",
    "# - data_series: data we want to lead/lag\n",
    "# - group_series: grouping we want to be the same for the lead/lag to be value\n",
    "# requires data_series and group_series already by sorted by group_series\n",
    "# so that all alike values of group_series are adjacent,\n",
    "# meaning group_series should look like:\n",
    "#    g_0\n",
    "#    g_0\n",
    "#    g_0\n",
    "#    g_0\n",
    "#    g_1\n",
    "#    g_1\n",
    "#    g_2\n",
    "#    g_2 \n",
    "#    ...\n",
    "# where g_i indicates the observation is in group i,\n",
    "# and once the first g_{i+1} appears no more g_i values appear\n",
    "# \n",
    "# lead_lag > 0 returns a data_series with values of data_series lead_lag rows ahead\n",
    "# as long as group_series remains the same, NaN if group different\n",
    "# lead_lag < 0 returns a data_series with values of data_series -lead_lag rows behind \n",
    "# (same as lead_lag rows ahead) as long as group_series remains the same, NaN if group different\n",
    "def safe_lead_lag(data_series,group_series,lead_lag): \n",
    "    df = pd.DataFrame({ 'data': data_series, 'group': group_series })\n",
    "    return df.groupby(['group'])['data'].shift(-lead_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "data_processor = MSLDataProcessor()\n",
    "unique_dates = data_processor.unique_dates()\n",
    "test_date = unique_dates[263]\n",
    "print(unique_dates.max())\n",
    "test_price_df = data_processor.price_df_for_date(test_date)\n",
    "test_price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "test_signal_df = data_processor.signal_df_for_date(test_date)\n",
    "test_signal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
